{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akhil\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:843: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# Import necessary packages\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.manifold import TSNE\n",
    "import re\n",
    "import matplotlib\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('all', halt_on_error=False)\n",
    "from nltk.corpus import stopwords # Import the stop word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read dataset\n",
    "questions = pd.read_csv('questions_data_for_assignment.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40146</td>\n",
       "      <td>79967</td>\n",
       "      <td>79968</td>\n",
       "      <td>If there was one movie that you would suggest ...</td>\n",
       "      <td>What is that one movie you will recommend some...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>157322</td>\n",
       "      <td>311105</td>\n",
       "      <td>311106</td>\n",
       "      <td>Is Rick Perry electable?</td>\n",
       "      <td>Where is Rick Perry?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>76441</td>\n",
       "      <td>151885</td>\n",
       "      <td>151886</td>\n",
       "      <td>Why am I getting my period so much?</td>\n",
       "      <td>Why am I getting my period twice a month?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>355648</td>\n",
       "      <td>696531</td>\n",
       "      <td>696532</td>\n",
       "      <td>Why absolute refractive index can never be les...</td>\n",
       "      <td>Why does a relative refractive index less than...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>136457</td>\n",
       "      <td>270171</td>\n",
       "      <td>270172</td>\n",
       "      <td>As kids develop into teenagers, do they someti...</td>\n",
       "      <td>Do psychopaths, as older children and teenager...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id    qid1    qid2                                          question1  \\\n",
       "0   40146   79967   79968  If there was one movie that you would suggest ...   \n",
       "1  157322  311105  311106                           Is Rick Perry electable?   \n",
       "2   76441  151885  151886                Why am I getting my period so much?   \n",
       "3  355648  696531  696532  Why absolute refractive index can never be les...   \n",
       "4  136457  270171  270172  As kids develop into teenagers, do they someti...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is that one movie you will recommend some...             1  \n",
       "1                               Where is Rick Perry?             0  \n",
       "2          Why am I getting my period twice a month?             0  \n",
       "3  Why does a relative refractive index less than...             0  \n",
       "4  Do psychopaths, as older children and teenager...             0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fill missing values\n",
    "questions['question2'].fillna('',inplace =True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating bag of words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function to convert a raw question to a string of words\n",
    "# The input is a single string (question), and the output is a single string (a preprocessed question)\n",
    "def review_to_words( raw_review ):\n",
    "    # 1. Remove HTML tags \n",
    "    review_text = BeautifulSoup(raw_review).get_text() \n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    # 4. Convert the stop words to a set (Searching is faster in a set than a list)\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    # 5. Remove stop words and lemmatize\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    #meaningful_words = [wordnet_lemmatizer.lemmatize(w) for w in words]\n",
    "    # 6. Join the words back into one string separated by space, and return the result.\n",
    "    return( \" \".join( meaningful_words ))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akhil\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file C:\\Users\\akhil\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 10000 of 300000\n",
      "\n",
      "Question 20000 of 300000\n",
      "\n",
      "Question 30000 of 300000\n",
      "\n",
      "Question 40000 of 300000\n",
      "\n",
      "Question 50000 of 300000\n",
      "\n",
      "Question 60000 of 300000\n",
      "\n",
      "Question 70000 of 300000\n",
      "\n",
      "Question 80000 of 300000\n",
      "\n",
      "Question 90000 of 300000\n",
      "\n",
      "Question 100000 of 300000\n",
      "\n",
      "Question 110000 of 300000\n",
      "\n",
      "Question 120000 of 300000\n",
      "\n",
      "Question 130000 of 300000\n",
      "\n",
      "Question 140000 of 300000\n",
      "\n",
      "Question 150000 of 300000\n",
      "\n",
      "Question 160000 of 300000\n",
      "\n",
      "Question 170000 of 300000\n",
      "\n",
      "Question 180000 of 300000\n",
      "\n",
      "Question 190000 of 300000\n",
      "\n",
      "Question 200000 of 300000\n",
      "\n",
      "Question 210000 of 300000\n",
      "\n",
      "Question 220000 of 300000\n",
      "\n",
      "Question 230000 of 300000\n",
      "\n",
      "Question 240000 of 300000\n",
      "\n",
      "Question 250000 of 300000\n",
      "\n",
      "Question 260000 of 300000\n",
      "\n",
      "Question 270000 of 300000\n",
      "\n",
      "Question 280000 of 300000\n",
      "\n",
      "Question 290000 of 300000\n",
      "\n",
      "Question 300000 of 300000\n",
      "\n",
      "Question 10000 of 300000\n",
      "\n",
      "Question 20000 of 300000\n",
      "\n",
      "Question 30000 of 300000\n",
      "\n",
      "Question 40000 of 300000\n",
      "\n",
      "Question 50000 of 300000\n",
      "\n",
      "Question 60000 of 300000\n",
      "\n",
      "Question 70000 of 300000\n",
      "\n",
      "Question 80000 of 300000\n",
      "\n",
      "Question 90000 of 300000\n",
      "\n",
      "Question 100000 of 300000\n",
      "\n",
      "Question 110000 of 300000\n",
      "\n",
      "Question 120000 of 300000\n",
      "\n",
      "Question 130000 of 300000\n",
      "\n",
      "Question 140000 of 300000\n",
      "\n",
      "Question 150000 of 300000\n",
      "\n",
      "Question 160000 of 300000\n",
      "\n",
      "Question 170000 of 300000\n",
      "\n",
      "Question 180000 of 300000\n",
      "\n",
      "Question 190000 of 300000\n",
      "\n",
      "Question 200000 of 300000\n",
      "\n",
      "Question 210000 of 300000\n",
      "\n",
      "Question 220000 of 300000\n",
      "\n",
      "Question 230000 of 300000\n",
      "\n",
      "Question 240000 of 300000\n",
      "\n",
      "Question 250000 of 300000\n",
      "\n",
      "Question 260000 of 300000\n",
      "\n",
      "Question 270000 of 300000\n",
      "\n",
      "Question 280000 of 300000\n",
      "\n",
      "Question 290000 of 300000\n",
      "\n",
      "Question 300000 of 300000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the number of questions based on the dataframe column size\n",
    "num_questions = questions['question1'].size\n",
    "\n",
    "# Initialize an empty list to hold clean questions\n",
    "clean_train_questions = []\n",
    "\n",
    "# Loop over each question in column question1 ; create an index i that goes from 0 to the length of the movie review list \n",
    "for i in range( 0, num_questions ):\n",
    "    # If the index is divisible by 10000, print a message\n",
    "    if( (i+1)%10000 == 0 ):\n",
    "        print(\"Question %d of %d\\n\" % ( i+1, num_questions ))                                                                    \n",
    "    clean_train_questions.append( review_to_words( questions['question1'][i] ))\n",
    "\n",
    "# Repeat same for the column question2\n",
    "for i in range( 0, num_questions ):\n",
    "    if( (i+1)%10000 == 0 ):\n",
    "        print(\"Question %d of %d\\n\" % ( i+1, num_questions ))                                                                    \n",
    "    clean_train_questions.append( review_to_words( questions['question2'][i] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"questions['question1'] = questions.apply(lambda x: x['question1'].split(), axis =1)\\nquestions['question2'] = questions.apply(lambda x: x['question2'].split(), axis =1)\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object (scikit-learn's bag of words tool.)  \n",
    "# Select 10000 most commonly occuring words\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = None, max_features = 10000) \n",
    "\n",
    "# Transform training data into feature vectors by giving input as list of strings\n",
    "train_data_features = vectorizer.fit_transform(clean_train_questions)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "train_data_features = train_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600000, 10000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the meta features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "# Initialize empty list to hold our meta features\n",
    "angles = []\n",
    "distances = []\n",
    "for i in range(0,num_questions):\n",
    "    angle = 0\n",
    "    dist = 0\n",
    "    for j in range(0,10000):\n",
    "        # Find angle by multiplying values of corresponding elements and summing it up \n",
    "        angle = angle + train_data_features[i][j] * train_data_features[i+300000][j]\n",
    "        # Find distance as root of sum of differneces between corresponding elements\n",
    "        dist = dist + (train_data_features[i][j] - train_data_features[i+300000][j])**2\n",
    "    angles.append(angle)\n",
    "    distances.append(math.sqrt(dist))\n",
    "    # Print after every 10000 elements\n",
    "    if(i%10000==0):\n",
    "        print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "questions['angles'] = angles\n",
    "questions['distances'] = distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Popping the columns with questions here since the questions are now unicode encoded and hence can not be read when \"read_csv\" command is used\n",
    "questions.pop('question1')\n",
    "questions.pop('question2')\n",
    "# Save final dataset as a new csv to file\n",
    "questions.to_csv('questions_final_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
